{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9372,"status":"ok","timestamp":1702459994377,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"xs5OEMhr6wop"},"outputs":[],"source":["import numpy as np\n","from keras import backend as keras\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","import tensorflow as tf\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","\n","from sklearn.model_selection import train_test_split\n","\n","from glob import glob\n","import cv2\n","\n","from PIL import Image\n","\n","import os\n","\n","from matplotlib import pyplot as plt\n","\n","import tensorflow.keras.backend as K"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20175,"status":"ok","timestamp":1702460026991,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"glaEMHBJ7phL","outputId":"f9b360e1-80cc-44ff-b020-0256898f57d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":570,"status":"ok","timestamp":1702460029361,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"qGo9nznl7dRP"},"outputs":[],"source":["os.chdir(\"drive/MyDrive/BioCV2023\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import custom_loss_function as clf\n","from keras.saving import *\n","import tensorflow.keras as keras\n","import warnings\n","\n","@tf.keras.saving.register_keras_serializable(name=\"weighted_categorical_crossentropy\")\n","def weighted_categorical_crossentropy(target, output, weights, axis=-1):\n","    target = tf.convert_to_tensor(target)\n","    output = tf.convert_to_tensor(output)\n","    target.shape.assert_is_compatible_with(output.shape)\n","    weights = tf.reshape(tf.convert_to_tensor(weights, dtype=target.dtype), (1,-1))\n","\n","    # Adjust the predictions so that the probability of\n","    # each class for every sample adds up to 1\n","    # This is needed to ensure that the cross entropy is\n","    # computed correctly.\n","    output = output / tf.reduce_sum(output, axis, True)\n","\n","    # Compute cross entropy from probabilities.\n","    epsilon_ = tf.constant(tf.keras.backend.epsilon(), output.dtype.base_dtype)     \n","    output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)     # evito di avere valori estremi (0 o 1)\n","    return -tf.reduce_sum(weights * target * tf.math.log(output), axis=axis)    # reduce perchè avrò 1 \"loss\" per pixel\n","\n","\n","# non è molto importante sapere come è costruita, sono cose tecniche anche inutili per il mio scopo\n","@tf.keras.saving.register_keras_serializable(name=\"WeightedCategoricalCrossentropy\")\n","class WeightedCategoricalCrossentropy:\n","    def __init__(\n","        self,\n","        weights,\n","        label_smoothing=0.0,\n","        axis=-1,\n","        name=\"weighted_categorical_crossentropy\",\n","        fn = None,\n","    ):\n","        \"\"\"Initializes `WeightedCategoricalCrossentropy` instance.\n","        Args:\n","          from_logits: Whether to interpret `y_pred` as a tensor of\n","            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n","            assume that `y_pred` contains probabilities (i.e., values in [0,\n","            1]).\n","          label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When >\n","            0, we compute the loss between the predicted labels and a smoothed\n","            version of the true labels, where the smoothing squeezes the labels\n","            towards 0.5.  Larger values of `label_smoothing` correspond to\n","            heavier smoothing.\n","          axis: The axis along which to compute crossentropy (the features\n","            axis).  Defaults to -1.\n","          name: Name for the op. Defaults to 'weighted_categorical_crossentropy'.\n","        \"\"\"\n","        super().__init__()\n","        self.weights = weights # tf.reshape(tf.convert_to_tensor(weights),(1,-1))\n","        self.label_smoothing = label_smoothing\n","        self.name = name\n","        self.fn = weighted_categorical_crossentropy if fn is None else fn\n","\n","    def __call__(self, y_true, y_pred, axis=-1):\n","        if isinstance(axis, bool):\n","            raise ValueError(\n","                \"`axis` must be of type `int`. \"\n","                f\"Received: axis={axis} of type {type(axis)}\"\n","            )\n","        y_pred = tf.convert_to_tensor(y_pred)\n","        y_true = tf.cast(y_true, y_pred.dtype)\n","        self.label_smoothing = tf.convert_to_tensor(self.label_smoothing, dtype=y_pred.dtype)\n","\n","        if y_pred.shape[-1] == 1:\n","            warnings.warn(\n","                \"In loss categorical_crossentropy, expected \"\n","                \"y_pred.shape to be (batch_size, num_classes) \"\n","                f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n","                \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n","                SyntaxWarning,\n","                stacklevel=2,\n","            )\n","\n","        def _smooth_labels():\n","            num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n","            return y_true * (1.0 - self.label_smoothing) + (self.label_smoothing / num_classes)\n","\n","        y_true = tf.__internal__.smart_cond.smart_cond(self.label_smoothing, _smooth_labels, lambda: y_true)\n","\n","        return tf.reduce_mean(self.fn(y_true, y_pred, self.weights, axis=axis))\n","    \n","    def get_config(self):\n","        config = {\"name\":self.name, \"weights\": self.weights, \"fn\": weighted_categorical_crossentropy}\n","\n","        # base_config = super().get_config()\n","        return dict(list(config.items()))\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        \"\"\"Instantiates a `Loss` from its config (output of `get_config()`).\n","        Args:\n","            config: Output of `get_config()`.\n","        \"\"\"\n","        return cls(**config)\n"]},{"cell_type":"markdown","metadata":{"id":"OsQtoKX16woy"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702460033181,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"DePjB9tV6wo2"},"outputs":[],"source":["IMAGE_SIZE = 256\n","num_classes = 5\n","batch = 4\n","LR = 1e-4\n","EPOCHS = 5\n","NUM_IMAGES = 522"]},{"cell_type":"markdown","metadata":{"id":"xeb_hiWj6wo4"},"source":["### Funzioni ausiliare\n","Qui sotto sono implementate funzioni per la creazione del dataset:\n","* caricamento percorsi\n","* divisione training/validation/test set\n","* caricamento e preprocessing immagini + maschere\n","* data augmentationc"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":930,"status":"ok","timestamp":1702463150805,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"m9jqEEWT6wo5"},"outputs":[],"source":["image_path = \"./Preprocessed_Set/T1DUAL/OutPhase/Images/\"  # da inserire\n","regex_images_paths = os.path.join(image_path, \"*.png\")\n","mask_path = \"./Preprocessed_Set/T1DUAL/OutPhase/Masks/\"   # da inserire\n","regex_masks_paths = os.path.join(mask_path, \"*.png\")\n","checkpoint_path = \"./checkpoints/checkpoint1\"\n","\n","# carica i percorsi dei dati separandoli in 80% training, 10% validation, 10% test\n","def load_data():\n","  images_paths = sorted(glob(regex_images_paths))\n","  masks_paths = sorted(glob(regex_masks_paths))\n","  train_x, tmp_x, train_y, tmp_y = train_test_split(images_paths, masks_paths, test_size=0.2, random_state=42, shuffle=True)\n","  valid_x, test_x, valid_y, test_y = train_test_split(tmp_x, tmp_y, test_size=0.5, random_state=42, shuffle=True)\n","\n","  return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1702463179903,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"tq6WVM7w6wo8"},"outputs":[],"source":["# carica l'immagine, normalizzandola a 0-1\n","def read_image(path):\n","    path = path.decode()\n","    x = np.array(Image.open(path))\n","    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE), interpolation=0)\n","    #x = x.astype(float)/255.0\n","    x = x/255.0\n","    return x\n","\n","# carica la maschera\n","def read_mask(path):\n","    path = path.decode()\n","    x = np.array(Image.open(path))\n","    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE), interpolation=0)\n","    x = K.cast(K.one_hot(x, num_classes=num_classes), tf.uint8)\n","    #x = np.expand_dims(x, axis=-1) #why would I need this?\n","    return x\n","\n","# mi carica una coppia immagine-maschera\n","def tf_parse(x, y):\n","    def _parse(x, y):\n","        x = read_image(x)\n","        y = read_mask(y)\n","        return x, y\n","\n","    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.uint8])\n","    x.set_shape([IMAGE_SIZE, IMAGE_SIZE])\n","    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, num_classes])\n","    return x, y\n","\n","# SIAMO SICURI CHE QUESTO FACCIA DATA AUGMENTATION? perchè sto semplicemente prendendo una coppia (x,y) e, con una certa probabilità, modifico x\n","# quindi 1. non modifico y\n","#        2. non genero nuovi esempi\n","# mi crea il dataset di training\n","def tf_dataset_train(x, y, batch=4):\n","    dataset = tf.data.Dataset.from_tensor_slices((x, y))    # creo il dataset con i percorsi...\n","    dataset = dataset.shuffle(buffer_size=500)\n","    dataset = dataset.repeat()\n","    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)    # e qui \"risolvo\" i percorsi\n","    dataset = dataset.batch(batch)\n","    #dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n","    # per data_augmentation nella libreria usa predict, invece di usare __call__, dove predict può essere eseguito su una batch alla volta e non su tutto l'input insieme\n","    # in questo caso credo sia indifferente, essendo una operazione \"statica\", che cioè non faccio durante l'allenamento del modello\n","    dataset = dataset.prefetch(1)       # per caricare in anticipo anche 1 elemento dopo\n","    return dataset\n","\n","def tf_dataset_valid(x, y, batch=batch):\n","    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n","    dataset = dataset.shuffle(buffer_size=50)\n","    dataset = dataset.repeat()\n","    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch)\n","    dataset = dataset.prefetch(1)\n","    return dataset\n"]},{"cell_type":"markdown","metadata":{"id":"M20Gcj2Q6wo-"},"source":["### Creazione modello Unet\n","Qui viene implementato il modello Unet. Da notare:\n","* non utilizzo come encoder una rete preesistente (tipicamente MobileNet) per semplicità\n","* l'output della CNN è una matrice di dimensione `(row_img, col_img, num_classes)`, dove, per ogni pixel _p_ per ogni classe _i_ assegno una probabilità (tramite softmax) al pixel di appartenere alla classe _i_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def unet(pretrained_weights = None,input_size = (256,256, 1)):\n","    inputs = Input(input_size)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","    drop4 = Dropout(0.5)(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","    drop5 = Dropout(0.5)(conv5)\n","\n","    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","    merge6 = concatenate([drop4,up6], axis = 3)   \n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","\n","    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","    merge7 = concatenate([conv3,up7], axis = 3)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([conv2,up8], axis = 3)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","\n","    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([conv1,up9], axis = 3)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    conv9 = Conv2D(5, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    conv10 = Conv2D(5, 1, activation = 'softmax')(conv9)\n","\n","    model = Model(inputs = inputs, outputs = conv10)\n","\n","    model.compile(optimizer = Adam(learning_rate = 1e-4), loss = clf.WeightedCategoricalCrossentropy([0.01, 10, 10, 10, 10]), metrics = ['accuracy'])   # posso usare metriche implementate da me (callbacks)\n","\n","    if(pretrained_weights):\n","        model.load_weights(pretrained_weights)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"zgE2Myzu6wpA"},"source":["## Creazione modello e training"]},{"cell_type":"markdown","metadata":{"id":"yD6sn6n76wpB"},"source":["### Creazione del dataset"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":904,"status":"ok","timestamp":1702463280437,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"9kZdjADn6wpB"},"outputs":[],"source":["(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data()\n","\n","train_dataset = tf_dataset_train(train_x, train_y)\n","valid_dataset = tf_dataset_valid(valid_x, valid_y)\n","test_dataset = tf_dataset_valid(test_x, test_y)"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1702463359583,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"LzV2CnTw6wpC","outputId":"d2a1ddec-9e60-4fdb-9d1a-ef286496233f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(None, 256, 256, 5) (None, 256, 256, 5)\n"]}],"source":["model = unet()\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='auto',\n","    save_best_only=True)\n","\n","#model.summary()\n","#tf.keras.utils.plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"IoaHPpNu6wpD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","105/105 [==============================] - 53s 403ms/step - loss: nan - accuracy: 0.9469 - val_loss: nan - val_accuracy: 0.9441\n","Epoch 2/5\n","105/105 [==============================] - 34s 323ms/step - loss: nan - accuracy: 0.9560 - val_loss: nan - val_accuracy: 0.9441\n","Epoch 3/5\n","105/105 [==============================] - 33s 319ms/step - loss: nan - accuracy: 0.9565 - val_loss: nan - val_accuracy: 0.9441\n","Epoch 4/5\n","105/105 [==============================] - 34s 320ms/step - loss: nan - accuracy: 0.9560 - val_loss: nan - val_accuracy: 0.9441\n","Epoch 5/5\n","105/105 [==============================] - 34s 320ms/step - loss: nan - accuracy: 0.9569 - val_loss: nan - val_accuracy: 0.9441\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7a1b0191bd60>"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["# Commencement of training\n","train_steps = len(train_x)//batch\n","valid_steps = len(valid_x)//batch\n","\n","if len(train_x) % batch != 0:   ## equivale a fare ceil() nelle equazioni di prima\n","    train_steps += 1\n","if len(valid_x) % batch != 0:\n","    valid_steps += 1\n","\n","model.fit(\n","    train_dataset,\n","    validation_data = valid_dataset,\n","    epochs=EPOCHS,\n","    steps_per_epoch=train_steps,\n","    validation_steps=valid_steps,\n","    callbacks=[model_checkpoint_callback]\n","    #callbacks=[model_checkpoint_callback, early_stopping, reduce_lr]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"elapsed":1674,"status":"ok","timestamp":1702460750486,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"yKkQcM7T6wpD","outputId":"27da33a3-2438-41ff-a4c1-28cb83011a58"},"outputs":[],"source":["def mask_from_prob(pred_mask):\n","    pred_mask = tf.argmax(pred_mask, axis=-1)\n","    return pred_mask\n","\n","def display_sample(display_list):\n","    plt.figure(figsize=(16, 16))\n","    title = ['Input Image', 'True Mask', 'Predicted Mask']\n","    for i in range(len(display_list)):\n","        plt.subplot(1, len(display_list), i+1)\n","        plt.title(title[i])\n","        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n","        plt.axis('off')\n","    plt.show()\n","\n","def show_predictions():\n","    prediction = model.predict(sample_image)\n","\n","    pred_mask = mask_from_prob(prediction[0])\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(sample_image[0], cmap=\"gray\")\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(sample_mask[0], cmap=\"gray\")\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(pred_mask, cmap=\"gray\")\n","\n","for image, mask in test_dataset.take(1):\n","    sample_image, sample_mask = image, mask\n","\n","show_predictions()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5pvuHkM6wpE"},"outputs":[],"source":["model = tf.keras.models.load_model(\"./checkpoints/checkpoint1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4481,"status":"ok","timestamp":1702456580791,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"J-2V6MnoQ0PK","outputId":"255fbc0b-c578-46c8-dae4-78072a572f37"},"outputs":[{"name":"stdout","output_type":"stream","text":["20/20 - 4s - loss: nan - accuracy: 0.9463 - 4s/epoch - 216ms/step\n"]}],"source":["loss, acc = model.evaluate(test_dataset, verbose=2, steps=20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for (x,y) in valid_dataset.take(1):\n","    imgs, msks = x,y\n","\n","plt.figure(figsize=(7,7))\n","predictions = model.predict(imgs)\n","pred_masks = [mask_from_prob(predictions[i]) for i in range(4)]\n","for i in range(3):\n","    plt.subplot(3,3,3*i+1)\n","    plt.imshow(imgs[i], cmap=\"gray\")\n","    plt.subplot(3,3,3*i+2)\n","    plt.imshow(msks[i], cmap=\"gray\")\n","    plt.subplot(3,3,3*i+3)\n","    plt.imshow(pred_masks[i], cmap=\"gray\")"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
