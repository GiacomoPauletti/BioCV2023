{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7150,"status":"ok","timestamp":1703067233974,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"_qr03vk9twEG"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-20 21:53:26.688881: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-20 21:53:26.766355: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-20 21:53:26.767817: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-20 21:53:37.219981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import numpy as np\n","from keras import backend as keras\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","import tensorflow as tf\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","\n","from sklearn.model_selection import train_test_split\n","\n","from glob import glob\n","import cv2\n","\n","from PIL import Image\n","\n","import os\n","\n","from matplotlib import pyplot as plt\n","\n","import tensorflow.keras.backend as K"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24915,"status":"ok","timestamp":1703067258886,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"QonQ_gEbtwES","outputId":"1f388a13-5372-4d19-c45d-b06a0dce477c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1703067258886,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"lvyWo4L-twEU"},"outputs":[],"source":["os.chdir(\"drive/MyDrive/BioCV2023\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1516,"status":"ok","timestamp":1703067260398,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"fZEs861etwEW"},"outputs":[],"source":["# code from https://medium.com/the-owl/weighted-categorical-cross-entropy-loss-in-keras-edaee1df44ee\n","import custom_loss_function as clf\n","from keras.saving import *\n","import tensorflow.keras as keras\n","import warnings\n","\n","@tf.keras.saving.register_keras_serializable(name=\"weighted_categorical_crossentropy\")\n","def weighted_categorical_crossentropy(target, output, weights, axis=-1):\n","    target = tf.convert_to_tensor(target)\n","    output = tf.convert_to_tensor(output)\n","    target.shape.assert_is_compatible_with(output.shape)\n","    weights = tf.reshape(tf.convert_to_tensor(weights, dtype=target.dtype), (1,-1))\n","\n","    # Adjust the predictions so that the probability of\n","    # each class for every sample adds up to 1\n","    # This is needed to ensure that the cross entropy is\n","    # computed correctly.\n","    output = output / tf.reduce_sum(output, axis, True)\n","\n","    # Compute cross entropy from probabilities.\n","    epsilon_ = tf.constant(tf.keras.backend.epsilon(), output.dtype.base_dtype)\n","    output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)     # avoiding extreme values (0 or 1)\n","    return -tf.reduce_sum(weights * target * tf.math.log(output), axis=axis)    # reduced sum so there is 1 loss value per pixel\n","\n","\n","@tf.keras.saving.register_keras_serializable(name=\"WeightedCategoricalCrossentropy\")\n","class WeightedCategoricalCrossentropy:\n","    def __init__(\n","        self,\n","        weights,\n","        label_smoothing=0.0,\n","        axis=-1,\n","        name=\"weighted_categorical_crossentropy\",\n","        fn = None,\n","    ):\n","        \"\"\"Initializes `WeightedCategoricalCrossentropy` instance.\n","        Args:\n","          from_logits: Whether to interpret `y_pred` as a tensor of\n","            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n","            assume that `y_pred` contains probabilities (i.e., values in [0,\n","            1]).\n","          label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When >\n","            0, we compute the loss between the predicted labels and a smoothed\n","            version of the true labels, where the smoothing squeezes the labels\n","            towards 0.5.  Larger values of `label_smoothing` correspond to\n","            heavier smoothing.\n","          axis: The axis along which to compute crossentropy (the features\n","            axis).  Defaults to -1.\n","          name: Name for the op. Defaults to 'weighted_categorical_crossentropy'.\n","        \"\"\"\n","        super().__init__()\n","        self.weights = weights # tf.reshape(tf.convert_to_tensor(weights),(1,-1))\n","        self.label_smoothing = label_smoothing\n","        self.name = name\n","        self.fn = weighted_categorical_crossentropy if fn is None else fn\n","\n","    def __call__(self, y_true, y_pred, axis=-1):\n","        if isinstance(axis, bool):\n","            raise ValueError(\n","                \"`axis` must be of type `int`. \"\n","                f\"Received: axis={axis} of type {type(axis)}\"\n","            )\n","        y_pred = tf.convert_to_tensor(y_pred)\n","        y_true = tf.cast(y_true, y_pred.dtype)\n","        self.label_smoothing = tf.convert_to_tensor(self.label_smoothing, dtype=y_pred.dtype)\n","\n","        if y_pred.shape[-1] == 1:\n","            warnings.warn(\n","                \"In loss categorical_crossentropy, expected \"\n","                \"y_pred.shape to be (batch_size, num_classes) \"\n","                f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n","                \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n","                SyntaxWarning,\n","                stacklevel=2,\n","            )\n","\n","        def _smooth_labels():\n","            num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n","            return y_true * (1.0 - self.label_smoothing) + (self.label_smoothing / num_classes)\n","\n","        y_true = tf.__internal__.smart_cond.smart_cond(self.label_smoothing, _smooth_labels, lambda: y_true)\n","\n","        return tf.reduce_mean(self.fn(y_true, y_pred, self.weights, axis=axis))\n","\n","    def get_config(self):\n","        config = {\"name\":self.name, \"weights\": self.weights, \"fn\": weighted_categorical_crossentropy}\n","\n","        # base_config = super().get_config()\n","        return dict(list(config.items()))\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        \"\"\"Instantiates a `Loss` from its config (output of `get_config()`).\n","        Args:\n","            config: Output of `get_config()`.\n","        \"\"\"\n","        return cls(**config)\n"]},{"cell_type":"markdown","metadata":{"id":"WhCveps-twEY"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1703068527842,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"oyu-iRlTtwEc"},"outputs":[],"source":["IMAGE_SIZE = 256\n","num_classes = 34    # now I have 34 classes since I'm detecting all possible borders\n","batch = 4\n","LR = 1e-4\n","EPOCHS = 20\n","NUM_IMAGES = 522"]},{"cell_type":"markdown","metadata":{"id":"tv-qwUQetwEd"},"source":["### Auxiliar functions\n","Here are implemented functions for creating the dataset\n","* loading paths\n","* division of training/validation/test sets\n","* loading and reshaping (+ normalizing) images and masks\n","* data augmentation"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":875,"status":"ok","timestamp":1703068530456,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"j1W9o72XtwEf"},"outputs":[],"source":["image_path = \"./Preprocessed_Set/T1DUAL/OutPhase/Images/\" \n","regex_images_paths = os.path.join(image_path, \"*.png\")\n","mask_path = \"./Preprocessed_Set/T1DUAL/OutPhase/Masks/\"   \n","regex_masks_paths = os.path.join(mask_path, \"*.png\")\n","checkpoint_path = \"./trained_models/model4/96.63cotp-96.54cott/\"\n","#  80% training, 10% validation, 10% test\n","def load_data():\n","  images_paths = sorted(glob(regex_images_paths))\n","  masks_paths = sorted(glob(regex_masks_paths))\n","  train_x, tmp_x, train_y, tmp_y = train_test_split(images_paths, masks_paths, test_size=0.2, random_state=42, shuffle=True)\n","  valid_x, test_x, valid_y, test_y = train_test_split(tmp_x, tmp_y, test_size=0.5, random_state=42, shuffle=True)\n","\n","  return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703068532198,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"oHlqtAjQtwEh"},"outputs":[],"source":["# loading and normalizing to range 0-1 an image\n","def read_image(path):\n","    path = path.decode()\n","    x = np.array(Image.open(path))\n","    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE), interpolation=0)\n","    #x = x.astype(float)/255.0\n","    x = x/255.0\n","    return x\n","\n","# loading the mask\n","def read_mask(path):\n","    path = path.decode()\n","    x = np.array(Image.open(path))\n","    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE), interpolation=0)\n","    x = K.cast(K.one_hot(x, num_classes=num_classes), tf.uint8)\n","    #x = np.expand_dims(x, axis=-1) \n","    return x\n","\n","# loading a image and mask\n","def tf_parse(x, y):\n","    def _parse(x, y):\n","        x = read_image(x)\n","        y = read_mask(y)\n","        return x, y\n","\n","    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.uint8])\n","    x.set_shape([IMAGE_SIZE, IMAGE_SIZE])\n","    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, num_classes])\n","    return x, y\n","\n","# creating training dataset\n","def tf_dataset_train(x, y, batch=4):\n","    dataset = tf.data.Dataset.from_tensor_slices((x, y))    # creating the dataset with the paths...\n","    dataset = dataset.shuffle(buffer_size=500)\n","    dataset = dataset.repeat()\n","    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)    # ... and resolving the paths\n","    dataset = dataset.batch(batch)\n","    #dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.prefetch(1)       # per caricare in anticipo anche 1 elemento dopo\n","    return dataset\n","\n","def tf_dataset_valid(x, y, batch=batch):\n","    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n","    dataset = dataset.shuffle(buffer_size=50)\n","    dataset = dataset.repeat()\n","    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch)\n","    dataset = dataset.prefetch(1)\n","    return dataset\n"]},{"cell_type":"markdown","metadata":{"id":"nlmC1G2htwEi"},"source":["### U-Net model\n","Here U-Net model is implemented. Notice that:\n","* CNN output matrix shape is `(row_img, col_img, num_classes)`, where for each pixel _p_ for each class _i_ a probability of belonging to class _i_ is assigned to pixel (through softmax) "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":480,"status":"ok","timestamp":1703068535620,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"qQ2EXebotwEj"},"outputs":[],"source":["def recall(y_true, y_pred):\n","  true_positives = K.sum(K.round(K.clip(y_true[:,:,2:] * y_pred[:,:,2:], 0, 1)))\n","  possible_positives = K.sum(K.round(K.clip(y_true[:,:,2:], 0, 1)))\n","  recall_keras = true_positives / (possible_positives + K.epsilon())\n","  return recall_keras\n","\n","\n","def precision(y_true, y_pred):\n","  true_positives = K.sum(K.round(K.clip(y_true[:,:,2:] * y_pred[:,:,2:], 0, 1)))\n","  predicted_positives = K.sum(K.round(K.clip(y_pred[:,:,2:], 0, 1)))\n","  precision_keras = true_positives / (predicted_positives + K.epsilon())\n","  return precision_keras\n","\n","def correct_over_total_pred(y_true, y_pred):\n","  correct_prediction = K.sum(K.round(K.clip(y_true[:,:,2:] * y_pred[:,:,2:], 0, 1)))\n","  total_prediction = K.sum(K.round(K.clip(y_pred[:,:,2:], 0, 1),))\n","  result = correct_prediction / (total_prediction + K.epsilon())\n","  return result\n","\n","def cotp(y_true, y_pred):\n","  return correct_over_total_pred(y_true, y_pred)\n","\n","def wrong_over_total_pred(y_true, y_pred):\n","  return 1 - correct_over_total_pred(y_true, y_pred)\n","\n","def correct_over_total_true(y_true, y_pred):\n","  true_positives = K.sum(K.round(K.clip(y_true[:,:,2:] * y_pred[:,:,2:], 0, 1)))\n","  possible_positives = K.sum(K.round(K.clip(y_true[:,:,2:], 0, 1)))\n","  recall_keras = true_positives / (possible_positives + K.epsilon())\n","  return recall_keras\n","\n","def cott(y_true, y_pred):\n","  return correct_over_total_true(y_true, y_pred)\n","\n","def true_over_total_pred(y_true, y_pred):\n","  true_positives = K.sum(K.round(K.clip(y_true[:,:,2:] * y_pred[:,:,2:], 0, 1)))\n","  predicted_positives = K.sum(K.round(K.clip(y_pred[:,:,2:], 0, 1)))\n","  precision_keras = true_positives / (predicted_positives + K.epsilon())\n","  return precision_keras\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":532,"status":"ok","timestamp":1703068646760,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"BuEcuu1vtwEk"},"outputs":[],"source":["skip_connection_names = [\"input_image\", \"conv1_conv\", \"conv2_block3_1_relu\", \"conv3_block4_1_relu\", \"conv4_block23_1_relu\"]\n","\n","def unet(input_size = (256,256, 1)):\n","    inputs = Input(input_size, name=\"input_image\")\n","    inputs0 = tf.image.grayscale_to_rgb(inputs)  # converting images to RGB since ResNet only receives RGB images\n","    base_model = tf.keras.applications.ResNet101V2(input_tensor=inputs0,include_top=False,weights='imagenet')\n","    base_model.trainable = False\n","\n","    base_model_output = base_model.get_layer(\"post_relu\").output\n","\n","    x_skip_1 = base_model.get_layer(skip_connection_names[-1]).output\n","    x_skip_2 = base_model.get_layer(skip_connection_names[-2]).output\n","    x_skip_3 = base_model.get_layer(skip_connection_names[-3]).output\n","    x_skip_4 = base_model.get_layer(skip_connection_names[-4]).output\n","    x_skip_5 = base_model.get_layer(skip_connection_names[-5]).output\n","    drop1 = Dropout(0.5)(base_model_output)\n","\n","    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop1))\n","    merge6 = concatenate([x_skip_1,up6], axis = 3)   # this is the where the skip connection arrives from the respective encoder layer\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","\n","    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","    merge7 = concatenate([x_skip_2,up7], axis = 3)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([x_skip_3,up8], axis = 3)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","\n","    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([x_skip_4,up9], axis = 3)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","\n","    up10 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv9))\n","    merge10 = concatenate([x_skip_5,up10], axis = 3)\n","    conv10 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge10)\n","    conv10 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv10)\n","    conv10 = Conv2D(num_classes, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv10)\n","    conv11 = Conv2D(num_classes, 1, activation = 'softmax')(conv10)\n","\n","    model = Model(inputs = inputs, outputs = conv11)\n","\n","    weights = [1, 3, 9, 14, 14, 14]\n","    border_weights = [18] * (33-5)\n","    weights = weights + border_weights\n","\n","    model.compile(optimizer = Adam(learning_rate = 1e-4), loss = clf.WeightedCategoricalCrossentropy(weights), metrics = ['accuracy', cotp, cott])   # posso usare metriche implementate da me (callbacks)\n","\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"czS1-_tstwEm"},"source":["## Model instanciation and training"]},{"cell_type":"markdown","metadata":{"id":"aN3w9hRhtwEm"},"source":["### Dataset creation"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":409,"status":"ok","timestamp":1703068649308,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"brK6c2WgtwEn"},"outputs":[],"source":["(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data()\n","\n","train_dataset = tf_dataset_train(train_x, train_y)\n","valid_dataset = tf_dataset_valid(valid_x, valid_y)\n","test_dataset = tf_dataset_valid(test_x, test_y)"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":4627,"status":"ok","timestamp":1703068655321,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"MSXfu1bjtwEn"},"outputs":[],"source":["model = unet()\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='auto',\n","    save_best_only=True)\n","\n","#model.summary()\n","#tf.keras.utils.plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":935743,"status":"ok","timestamp":1703069591062,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"-E2cTLYVtwEo","outputId":"e0671551-6606-4366-8b7c-b3d3410cc4fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","105/105 [==============================] - 72s 588ms/step - loss: 4.1705 - accuracy: 0.7293 - cotp: 0.8455 - cott: 0.5773 - val_loss: 2.9842 - val_accuracy: 0.9000 - val_cotp: 0.9748 - val_cott: 0.7553\n","Epoch 2/20\n","105/105 [==============================] - 56s 535ms/step - loss: 2.1259 - accuracy: 0.9141 - cotp: 0.9553 - cott: 0.8654 - val_loss: 2.2803 - val_accuracy: 0.9096 - val_cotp: 0.9530 - val_cott: 0.8836\n","Epoch 3/20\n","105/105 [==============================] - 60s 575ms/step - loss: 1.7267 - accuracy: 0.9202 - cotp: 0.9582 - cott: 0.8879 - val_loss: 1.6869 - val_accuracy: 0.9123 - val_cotp: 0.9623 - val_cott: 0.8766\n","Epoch 4/20\n","105/105 [==============================] - 57s 550ms/step - loss: 1.3218 - accuracy: 0.9272 - cotp: 0.9582 - cott: 0.9053 - val_loss: 1.3942 - val_accuracy: 0.9347 - val_cotp: 0.9689 - val_cott: 0.9051\n","Epoch 5/20\n","105/105 [==============================] - 57s 547ms/step - loss: 1.0769 - accuracy: 0.9352 - cotp: 0.9586 - cott: 0.9181 - val_loss: 1.1442 - val_accuracy: 0.9352 - val_cotp: 0.9584 - val_cott: 0.9154\n","Epoch 6/20\n","105/105 [==============================] - 59s 565ms/step - loss: 0.9442 - accuracy: 0.9405 - cotp: 0.9573 - cott: 0.9280 - val_loss: 1.1270 - val_accuracy: 0.9359 - val_cotp: 0.9577 - val_cott: 0.9184\n","Epoch 7/20\n","105/105 [==============================] - 57s 546ms/step - loss: 0.8044 - accuracy: 0.9470 - cotp: 0.9573 - cott: 0.9389 - val_loss: 0.9974 - val_accuracy: 0.9470 - val_cotp: 0.9590 - val_cott: 0.9366\n","Epoch 8/20\n","105/105 [==============================] - 19s 179ms/step - loss: 0.7023 - accuracy: 0.9523 - cotp: 0.9590 - cott: 0.9469 - val_loss: 0.9474 - val_accuracy: 0.9449 - val_cotp: 0.9566 - val_cott: 0.9360\n","Epoch 9/20\n","105/105 [==============================] - 58s 557ms/step - loss: 0.6546 - accuracy: 0.9541 - cotp: 0.9590 - cott: 0.9499 - val_loss: 0.8748 - val_accuracy: 0.9490 - val_cotp: 0.9557 - val_cott: 0.9437\n","Epoch 10/20\n","105/105 [==============================] - 58s 556ms/step - loss: 0.5889 - accuracy: 0.9584 - cotp: 0.9618 - cott: 0.9553 - val_loss: 0.8566 - val_accuracy: 0.9541 - val_cotp: 0.9631 - val_cott: 0.9462\n","Epoch 11/20\n","105/105 [==============================] - 57s 545ms/step - loss: 0.5573 - accuracy: 0.9605 - cotp: 0.9634 - cott: 0.9579 - val_loss: 0.8367 - val_accuracy: 0.9567 - val_cotp: 0.9618 - val_cott: 0.9529\n","Epoch 12/20\n","105/105 [==============================] - 57s 544ms/step - loss: 0.5414 - accuracy: 0.9607 - cotp: 0.9632 - cott: 0.9583 - val_loss: 0.7937 - val_accuracy: 0.9569 - val_cotp: 0.9618 - val_cott: 0.9530\n","Epoch 13/20\n","105/105 [==============================] - 19s 179ms/step - loss: 0.5177 - accuracy: 0.9614 - cotp: 0.9636 - cott: 0.9592 - val_loss: 0.7591 - val_accuracy: 0.9568 - val_cotp: 0.9603 - val_cott: 0.9541\n","Epoch 14/20\n","105/105 [==============================] - 19s 179ms/step - loss: 0.4671 - accuracy: 0.9639 - cotp: 0.9654 - cott: 0.9623 - val_loss: 0.7760 - val_accuracy: 0.9561 - val_cotp: 0.9590 - val_cott: 0.9537\n","Epoch 15/20\n","105/105 [==============================] - 59s 566ms/step - loss: 0.4397 - accuracy: 0.9660 - cotp: 0.9672 - cott: 0.9646 - val_loss: 0.8049 - val_accuracy: 0.9572 - val_cotp: 0.9597 - val_cott: 0.9551\n","Epoch 16/20\n","105/105 [==============================] - 57s 542ms/step - loss: 0.4255 - accuracy: 0.9672 - cotp: 0.9683 - cott: 0.9660 - val_loss: 0.7793 - val_accuracy: 0.9579 - val_cotp: 0.9601 - val_cott: 0.9561\n","Epoch 17/20\n","105/105 [==============================] - 57s 550ms/step - loss: 0.4196 - accuracy: 0.9678 - cotp: 0.9687 - cott: 0.9667 - val_loss: 0.8134 - val_accuracy: 0.9611 - val_cotp: 0.9633 - val_cott: 0.9592\n","Epoch 18/20\n","105/105 [==============================] - 19s 178ms/step - loss: 0.3813 - accuracy: 0.9693 - cotp: 0.9700 - cott: 0.9685 - val_loss: 0.7813 - val_accuracy: 0.9602 - val_cotp: 0.9619 - val_cott: 0.9588\n","Epoch 19/20\n","105/105 [==============================] - 19s 178ms/step - loss: 0.3714 - accuracy: 0.9696 - cotp: 0.9702 - cott: 0.9689 - val_loss: 0.7390 - val_accuracy: 0.9601 - val_cotp: 0.9620 - val_cott: 0.9587\n","Epoch 20/20\n","105/105 [==============================] - 20s 192ms/step - loss: 0.3492 - accuracy: 0.9714 - cotp: 0.9719 - cott: 0.9708 - val_loss: 0.7789 - val_accuracy: 0.9608 - val_cotp: 0.9630 - val_cott: 0.9590\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7e95a7cf4370>"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["# Commencement of training\n","train_steps = len(train_x)//batch\n","valid_steps = len(valid_x)//batch\n","\n","if len(train_x) % batch != 0:   ## equivalent of doing ceil in the code before\n","    train_steps += 1\n","if len(valid_x) % batch != 0:\n","    valid_steps += 1\n","\n","model.fit(\n","    train_dataset,\n","    validation_data = valid_dataset,\n","    epochs=EPOCHS,\n","    steps_per_epoch=train_steps,\n","    validation_steps=valid_steps,\n","    callbacks=[model_checkpoint_callback]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Postprocessing: border mapped to original classes and image plotting"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1703069611008,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"stDMQmrDtwEp"},"outputs":[],"source":["# used to detect borders in preprocessing and now for mapping borders to the 6 original classes\n","border_type = {\n","    0: [0,0, 6, 7, 8, 9],   # background border with (by index) 0) background, 1) body, 2) liver, 3) right kidney, 4) left kidney, 5) spleen\n","    1: [0,0,10,11,12,13],   # body border with ... (same)\n","    2: [14,15,0,16,17,18],  # liver border with ... (same)\n","    3: [19,20,21,0,22,23],  # right kidney border with ... (same)\n","    4: [24,25,26,27,0,28],  # left kidney border with ... (same)\n","    5: [29,30,31,32,33,0]   # spleen border with ... (same)\n","}\n","\n","# creating a map for mapping borders to the 6 original classes, by \"inverting\" border_type map\n","to_real_class = dict([[0,0],[1,1],[2,2],[3,3],[4,4],[5,5]])\n","for (key,values) in border_type.items():\n","    for value in values:\n","        if value == 0: continue\n","        to_real_class[value] = key"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":545,"status":"ok","timestamp":1703069618534,"user":{"displayName":"Giacomo Pauletti","userId":"12169296531290997792"},"user_tz":-60},"id":"kuDQjxxdtwEp"},"outputs":[],"source":["def mask_from_prob(mask):\n","    def _mask_from_prob(mask):\n","        mask = tf.argmax(mask, axis=-1)\n","        return mask\n","\n","    msk = _mask_from_prob(mask)\n","    msk = np.array(msk)\n","    for row in range(msk.shape[0]):\n","        for col in range(msk.shape[1]):\n","            msk[row][col] = to_real_class[msk[row][col]]\n","    return msk\n","\n","def display_sample(display_list):\n","    plt.figure(figsize=(16, 16))\n","    title = ['Input Image', 'True Mask', 'Predicted Mask']\n","    for i in range(len(display_list)):\n","        plt.subplot(1, len(display_list), i+1)\n","        plt.title(title[i])\n","        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n","        plt.axis('off')\n","    plt.show()\n","\n","def show_predictions(i=0):\n","    prediction = model.predict(sample_image)\n","    true_mask = mask_from_prob(sample_mask[i])\n","    pred_mask = mask_from_prob(prediction[i])\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(sample_image[i], cmap=\"gray\")\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(true_mask, cmap=\"gray\")\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(pred_mask, cmap=\"gray\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for image, mask in test_dataset.take(1):\n","    sample_image, sample_mask = image, mask\n","\n","prediction = model.predict(sample_image)\n","plt.figure(figsize=(10,10))\n","rows = prediction.shape[0]\n","for i in range(rows):\n","  true_mask = mask_from_prob(sample_mask[i])\n","  pred_mask = mask_from_prob(prediction[i])\n","  plt.subplot(rows, 3, 3*i +1)\n","  plt.imshow(sample_image[i], cmap=\"gray\")\n","  plt.subplot(rows, 3, 3*i + 2)\n","  plt.imshow(true_mask, cmap=\"gray\")\n","  plt.subplot(rows, 3, 3*i + 3)\n","  plt.imshow(pred_mask, cmap=\"gray\")"]},{"cell_type":"markdown","metadata":{},"source":["## Model loading for future use"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Zfqr8oEotwEq"},"outputs":[],"source":["model = tf.keras.models.load_model(checkpoint_path, custom_objects={'cotp':cotp, 'cott':cott}, compile=False)\n","weights = [1, 3, 9, 14, 14, 14]     # background, body, liver, kidneys and spleen weights\n","border_weights = [18] * (33-5)      # organ borders weights\n","weights = weights + border_weights\n","model.compile(optimizer = Adam(learning_rate = 1e-4), loss = WeightedCategoricalCrossentropy(weights), metrics = ['accuracy', cotp, cott])   "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"_5JxciEhtwEr","outputId":"e7b9a953-a654-4240-f102-35c186a6eecd"},"outputs":[{"name":"stdout","output_type":"stream","text":["20/20 - 40s - loss: 0.9038 - accuracy: 0.9672 - cotp: 0.9676 - cott: 0.9666 - 40s/epoch - 2s/step\n"]}],"source":["result = model.evaluate(test_dataset, verbose=2, steps=20)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
